{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys\n",
    "import pickle as pkl\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "import itertools\n",
    "from math import ceil \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# load  preprocessed dataset\n",
    "data=pd.read_csv(\"Train.csv\")\n",
    "\n",
    "data_new=data.as_matrix()\n",
    "\n",
    "X=data_new[:,:-1]\n",
    "Y=data_new[:,-1]\n",
    "\n",
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, Y, test_size=0.30,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit the GPU memory this program is using\n",
    "config  = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "keras.backend.set_session(tf.Session(config = config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(verbosity=3, \n",
    "                      scoring=\"balanced_accuracy\", \n",
    "                      random_state=23, \n",
    "                      periodic_checkpoint_folder=\"tpot_results\", \n",
    "                      n_jobs=1, \n",
    "                      generations=5, \n",
    "                      population_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=300.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #6 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #8 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #10 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #16 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #19 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #21 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #24 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #26 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #28 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #31 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #33 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #35 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #37 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #39 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #41 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #43 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #45 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #48 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #54 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #60 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #62 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #64 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #66 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #69 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #74 due to time out. Continuing to the next pipeline.\n",
      "Created new folder to save periodic pipeline: tpot_results\n",
      "Saving periodic pipeline from pareto front to tpot_results\\pipeline_gen_1_idx_0_2020.02.11_17-46-11.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #80 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #85 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #87 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #92 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #94 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #96 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #98 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #100 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #102 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #104 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #110 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #112 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #114 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #121 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #123 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #125 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #128 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #130 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #132 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #134 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #137 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #139 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #142 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #146 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #148 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #151 due to time out. Continuing to the next pipeline.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.9975829925234446\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Skipped pipeline #157 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #159 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #161 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #164 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #175 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #178 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #185 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #195 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #201 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #204 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #206 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #210 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #212 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #214 due to time out. Continuing to the next pipeline.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.9975829925234446\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "-2\t0.9988659540413212\tDecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Saving periodic pipeline from pareto front to tpot_results\\pipeline_gen_2_idx_1_2020.02.11_22-54-09.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #223 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #227 due to time out. Continuing to the next pipeline.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped pipeline #238 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #240 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #243 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #247 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #253 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #258 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #262 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #264 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #266 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #270 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #276 due to time out. Continuing to the next pipeline.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.9975829925234446\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "-2\t0.9988659540413212\tDecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Skipped pipeline #285 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #295 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #299 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #307 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #309 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #313 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #317 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #320 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #324 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #327 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #329 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #334 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #340 due to time out. Continuing to the next pipeline.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.9975829925234446\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "-2\t0.9988659540413212\tDecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=3)\n",
      "\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Periodic pipeline was not saved, probably saved before...\n",
      "Skipped pipeline #349 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #353 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #356 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #361 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #364 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #366 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #370 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #380 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #389 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #394 due to time out. Continuing to the next pipeline.\n",
      "Skipped pipeline #402 due to time out. Continuing to the next pipeline.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.9977529546114491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=16)\n",
      "-2\t0.9988887275364388\tDecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=4, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=10, DecisionTreeClassifier__min_samples_leaf=10, DecisionTreeClassifier__min_samples_split=16)\n",
      "\n",
      "Saving periodic pipeline from pareto front to tpot_results\\pipeline_gen_5_idx_0_2020.02.12_05-26-21.py\n",
      "Saving periodic pipeline from pareto front to tpot_results\\pipeline_gen_5_idx_1_2020.02.12_05-26-21.py\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "               disable_update_check=False, early_stop=None, generations=5,\n",
       "               max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "               mutation_rate=0.9, n_jobs=1, offspring_size=None,\n",
       "               periodic_checkpoint_folder='tpot_results', population_size=50,\n",
       "               random_state=23, scoring='balanced_accuracy', subsample=1.0,\n",
       "               template=None, use_dask=False, verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9991391395981906\n"
     ]
    }
   ],
   "source": [
    "print(tpot.score(x_test, y_test))\n",
    "tpot.export('tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########training the exported tpot_pipeline.py file#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "\n",
    "tpot_data = pd.read_csv('train.csv')\n",
    "features = tpot_data.drop('Label', axis=1).values\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['Label'].values, random_state=23)\n",
    "\n",
    "# Average CV score on the training set was:0.9988887275364388\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=10, min_samples_leaf=4, min_samples_split=20)),\n",
    "    DecisionTreeClassifier(criterion=\"entropy\", max_depth=10, min_samples_leaf=10, min_samples_split=16)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(training_features, training_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 99.91672364857381\n",
      "Classification Report:-\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    166891\n",
      "           1       1.00      1.00      1.00     94888\n",
      "\n",
      "    accuracy                           1.00    261779\n",
      "   macro avg       1.00      1.00      1.00    261779\n",
      "weighted avg       1.00      1.00      1.00    261779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy\",exported_pipeline.score(testing_features, testing_target)*100)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "print(\"Classification Report:-\\n\",classification_report(testing_target,results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[166756    135]\n",
      " [    83  94805]]\n",
      "\n",
      "Total number of true positives(Attack identified as Attack)) 94805\n",
      "Total number of false negatives (Attack identified as Benign) 83\n",
      "Total number of false positives(Benign identified as Attack) 135\n",
      "Total number of true negatives (Benign identified as Benign) 166756\n",
      "Real positives: 94888\n",
      "Real negatives: 166891\n",
      "Predicted positives: 94940\n",
      "Predicted negatives: 166839 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit\n",
    "cnf_matrix1 = confusion_matrix(testing_target,results)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(cnf_matrix1)\n",
    "tn = cnf_matrix1[0][0]\n",
    "fp = cnf_matrix1[0][1]\n",
    "fn = cnf_matrix1[1][0]\n",
    "tp = cnf_matrix1[1][1]\n",
    "print(\"\\nTotal number of true positives(Attack identified as Attack))\", tp)\n",
    "print(\"Total number of false negatives (Attack identified as Benign)\",fn)\n",
    "print(\"Total number of false positives(Benign identified as Attack)\",fp)\n",
    "print(\"Total number of true negatives (Benign identified as Benign)\",tn)\n",
    "\n",
    "rp=tp+fn\n",
    "rn=fp+tn\n",
    "pp=tp+fp\n",
    "pn=tn+fn\n",
    "\n",
    "print(\"Real positives:\",rp)\n",
    "print(\"Real negatives:\",rn)\n",
    "print(\"Predicted positives:\",pp)\n",
    "print(\"Predicted negatives:\",pn,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(exported_pipeline, open(\"TPOT.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############prediction on synthesized test attacks##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#predict on synthesized Slow httptest DoS attack\n",
    "\n",
    "test_data=pd.read_csv('ShttpAA1.csv')\n",
    "test_data_new=test_data.as_matrix()\n",
    "\n",
    "X_test1=test_data_new[:,:-1] \n",
    "Y_test1=test_data_new[:,-1]\n",
    "\n",
    "#predict on synthesized Slowloris DoS attack\n",
    "\n",
    "test_data2=pd.read_csv('SlowAA1.csv')\n",
    "test_data_new2=test_data2.as_matrix()\n",
    "\n",
    "X_test2=test_data_new2[:,:-1]\n",
    "Y_test2=test_data_new2[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy for Slow httptest=  69.13220733838091\n",
      "Classification Report for Slow httptest:-\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.32      1.00      0.48       244\n",
      "         1.0       1.00      0.64      0.78      1473\n",
      "\n",
      "    accuracy                           0.69      1717\n",
      "   macro avg       0.66      0.82      0.63      1717\n",
      "weighted avg       0.90      0.69      0.74      1717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model from file\n",
    "model2 = pickle.load(open(\"TPOT.dat\", \"rb\"))\n",
    "\n",
    "Y_pred1 = model2.predict(X_test1)\n",
    "print(\"Testing Accuracy for Slow httptest= \", model2.score(X_test1,Y_test1)*100)\n",
    "print(\"Classification Report for Slow httptest:-\\n\",classification_report(Y_test1,Y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy for Slowloris=  100.0\n",
      "Classification Report for Slowloris:-\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       244\n",
      "         1.0       1.00      1.00      1.00       268\n",
      "\n",
      "    accuracy                           1.00       512\n",
      "   macro avg       1.00      1.00      1.00       512\n",
      "weighted avg       1.00      1.00      1.00       512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred2 = model2.predict(X_test2)\n",
    "print(\"Testing Accuracy for Slowloris= \", model2.score(X_test2,Y_test2)*100)\n",
    "print(\"Classification Report for Slowloris:-\\n\",classification_report(Y_test2,Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Slow httptest:\n",
      "[[244   0]\n",
      " [530 943]]\n",
      "\n",
      "Total number of true positives(Attack identified as Attack)) 943\n",
      "Total number of false negatives (Attack identified as Benign) 530\n",
      "Total number of false positives(Benign identified as Attack) 0\n",
      "Total number of true negatives (Benign identified as Benign) 244\n",
      "Real positives: 1473\n",
      "Real negatives: 244\n",
      "Predicted positives: 943\n",
      "Predicted negatives: 774 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix for Slow httptest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit\n",
    "cm1 = confusion_matrix(Y_test1, Y_pred1)\n",
    "print(\"Confusion matrix for Slow httptest:\")\n",
    "print(cm1)\n",
    "tn = cm1[0][0]\n",
    "fp = cm1[0][1]\n",
    "fn = cm1[1][0]\n",
    "tp = cm1[1][1]\n",
    "print(\"\\nTotal number of true positives(Attack identified as Attack))\", tp)\n",
    "print(\"Total number of false negatives (Attack identified as Benign)\",fn)\n",
    "print(\"Total number of false positives(Benign identified as Attack)\",fp)\n",
    "print(\"Total number of true negatives (Benign identified as Benign)\",tn)\n",
    "\n",
    "rp=tp+fn\n",
    "rn=fp+tn\n",
    "pp=tp+fp\n",
    "pn=tn+fn\n",
    "\n",
    "print(\"Real positives:\",rp)\n",
    "print(\"Real negatives:\",rn)\n",
    "print(\"Predicted positives:\",pp)\n",
    "print(\"Predicted negatives:\",pn,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Slowloris:\n",
      "[[244   0]\n",
      " [  0 268]]\n",
      "\n",
      "Total number of true positives(Attack identified as Attack)) 268\n",
      "Total number of false negatives (Attack identified as Benign) 0\n",
      "Total number of false positives(Benign identified as Attack) 0\n",
      "Total number of true negatives (Benign identified as Benign) 244\n",
      "Real positives: 268\n",
      "Real negatives: 244\n",
      "Predicted positives: 268\n",
      "Predicted negatives: 244 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix for Slowloris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,ShuffleSplit\n",
    "cm2 = confusion_matrix(Y_test2, Y_pred2)\n",
    "print(\"Confusion matrix for Slowloris:\")\n",
    "print(cm2)\n",
    "tn = cm2[0][0]\n",
    "fp = cm2[0][1]\n",
    "fn = cm2[1][0]\n",
    "tp = cm2[1][1]\n",
    "print(\"\\nTotal number of true positives(Attack identified as Attack))\", tp)\n",
    "print(\"Total number of false negatives (Attack identified as Benign)\",fn)\n",
    "print(\"Total number of false positives(Benign identified as Attack)\",fp)\n",
    "print(\"Total number of true negatives (Benign identified as Benign)\",tn)\n",
    "\n",
    "rp=tp+fn\n",
    "rn=fp+tn\n",
    "pp=tp+fp\n",
    "pn=tn+fn\n",
    "\n",
    "print(\"Real positives:\",rp)\n",
    "print(\"Real negatives:\",rn)\n",
    "print(\"Predicted positives:\",pp)\n",
    "print(\"Predicted negatives:\",pn,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
